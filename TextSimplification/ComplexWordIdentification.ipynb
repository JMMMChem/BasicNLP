{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ComplexWordIdentification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/TextSimplification/blob/master/ComplexWordIdentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mDPS6_LdxTN",
        "colab_type": "text"
      },
      "source": [
        "# Complex Word Identification\n",
        "\n",
        "Our goal is to develop a machine learning based system to determine if a word is complex (difficult) or simple (easy). You can find more information about this task at https://sites.google.com/view/cwisharedtask2018/\n",
        "\n",
        "\n",
        "We are going to use the dataset provided by the Complex Word Identification (CWI) shared task https://sites.google.com/view/cwisharedtask2018/datasets?authuser=0. This dataset contains a list of words, where each word is classifies as 0 (simple) or as 1 (complex). This tasks is defined for three different languages: English, Spanish and German. \n",
        "\n",
        "\n",
        "In this notebook, we will only work with a subset of the English dataset. This dataset consists of two files:\n",
        "- **Wikipedia_Train1.tsv** containing our training instances.\n",
        "- **Wikipedia_Dev1.tsv** containing the instances that we will use to evaluate our system.\n",
        "\n",
        "In this notebook, we will learn the following issues:\n",
        "- read the dataset\n",
        "- represent each word using a set of features useful for the task. \n",
        "- train a SVM model \n",
        "- Use the SVM model to predict the classes for the test dataset (**Wikipedia_Dev1.tsv**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfxabSXhdEO3",
        "colab_type": "text"
      },
      "source": [
        "First, we must the local folder of our google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH2BA9-yd8p1",
        "colab_type": "code",
        "outputId": "82c25b1f-4e8e-46f7-8d9e-452137d1cb67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqUt1bq8c5cw",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset\n",
        "\n",
        "Please, read the file **README.md** (data/README.md), which explains the format of the dataset. \n",
        "\n",
        "Each line in **Wikipedia_Train1.tsv** provides information about a word. \n",
        "For example:\n",
        "\n",
        "\n",
        "**3Z8UJEJOCZEG603II1EL4BE2PV593A\tSyrian troops shelled a rebel-held town on Monday, sparking intense clashes that sent bloodied victims flooding into hospitals and clinics, activists said.\t7\t13\ttroops\t10\t10\t0\t0\t0\t0.0**\n",
        "\n",
        "The fields for each word are:\n",
        "\n",
        "*   An identifier for the text. (for instance 3Z8UJEJOCZEG603II1EL4BE2PV593A)\n",
        "*   The text where the word occurs. (for instance *Syrian troops...., activists said.*)\n",
        "*   The start and end positions (offsets) of the word in the text. (For instance: 7 13).\n",
        "*   The word to be classified: *troops*.\n",
        "*   The 6th columns shows the number of native annotators who reviewed the word.\n",
        "*   The 7th columns shows the number of non-native annotators who reviewed the word.\n",
        "*   The 8th columns shows the number of native annotators who classified the word as complex (with the label 1).\n",
        "*   The 9th columns shows the number of non-native annotators who classified the word as complex (with the label 1).\n",
        "*   The 10th columns is 1 if the word was classified as complex, and 0 if the word was classified as simple. \n",
        "*   The 11th columns provides the probability of the word to be classified as complex. \n",
        "\n",
        "In this approach, we are not going to exploit the 6th-9th columns. \n",
        "\n",
        "The dataset could be readed using different methods. I recommend you to use **csv** package. \n",
        "\n",
        "The following code shows how to read the dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Gj1EvOdxTO",
        "colab_type": "code",
        "outputId": "de368d5b-67b9-4728-9e46-7d8eafd0c8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import csv #package to work with tsv files\n",
        "sst_home='drive/My Drive/Colab Notebooks/'\n",
        "#Modify your folder\n",
        "sst_home += 'TESI/6-TextSimplification/'\n",
        "path=sst_home+'data/Wikipedia_Train1.tsv'\n",
        "\n",
        "tsvin=open(path,'rb') \n",
        "tsvin = csv.reader(tsvin, delimiter='\\t')\n",
        "\n",
        "i=0\n",
        "for row in tsvin:\n",
        "    id=row[0] # id test\n",
        "    sentence=row[1] #sentence text\n",
        "    start=row[2] #start offset \n",
        "    end=row[3] #end offset + 1\n",
        "    word=row[4] #word to be classified\n",
        "    \n",
        "    \n",
        "    nat=row[5] #number of native annotators\n",
        "    nonnat=row[6] #number of non-native annotators\n",
        "    nat1=row[7] #number of native annotators who classified the words as 1\n",
        "    nonnat1=row[8]#number of non-native annotators who classified the words as 1\n",
        "\n",
        "    \n",
        "    class_word=row[9] #class: 1 (complex) or 0 (simple)\n",
        "    probability=row[10] #(total annotators who assigned 1/ total anotadores)\n",
        "\n",
        "    #we show the 10 first words\n",
        "    print('id',id)\n",
        "    print('sentence',sentence)\n",
        "    print('start',start)\n",
        "    print('end',end)\n",
        "    print('word',word)\n",
        "    #print(\"anotadores\",nat,nonnat,nat1,nonnat1)\n",
        "    #print(\"clase y probabilidad\", class_word,probability)\n",
        "    print(\"clase:\", class_word)\n",
        "    \n",
        "    print('\\n')\n",
        "    i+=1\n",
        "    if i==10:\n",
        "        break\n",
        "    \n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '0')\n",
            "('end', '8')\n",
            "('word', 'Normally')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '28')\n",
            "('end', '34')\n",
            "('word', 'passed')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '15')\n",
            "('end', '19')\n",
            "('word', 'land')\n",
            "('clase:', '0')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '43')\n",
            "('end', '49')\n",
            "('word', 'future')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '50')\n",
            "('end', '61')\n",
            "('word', 'generations')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '76')\n",
            "('end', '86')\n",
            "('word', 'recognizes')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '91')\n",
            "('end', '100')\n",
            "('word', 'community')\n",
            "('clase:', '0')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '104')\n",
            "('end', '115')\n",
            "('word', 'traditional')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '135')\n",
            "('end', '142')\n",
            "('word', 'country')\n",
            "('clase:', '1')\n",
            "\n",
            "\n",
            "('id', '3XU9MCX6VODXPI3L8I02CM94TFB2R7')\n",
            "('sentence', \"Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .\")\n",
            "('start', '116')\n",
            "('end', '126')\n",
            "('word', 'connection')\n",
            "('clase:', '0')\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83PGq0NdxTV",
        "colab_type": "text"
      },
      "source": [
        "## Feature set for word complex indentification\n",
        "\n",
        "Now we are going to represent each word to be classified with a set of features, which should be useful for the task of CWI. So each word will be represented by a vector of features and its class (1 or 0). \n",
        "\n",
        "We will start with a small set of features, which have proved valuable for this task:\n",
        "<ul>\n",
        "<li>Length and number of syllables of a word. Complex words usually have a  longer length.\n",
        "<li>Length of the sentence where the word occurs. \n",
        "A long sentence could indicate that the word is complex, but not always.</li>\n",
        "</ul>\n",
        "\n",
        "The following cell shows how to obtain these features for the first 50 words in the dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRr_lXWdxTX",
        "colab_type": "code",
        "outputId": "ceed535f-2b15-4a01-a7ab-8d6ca3a00879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "#a function to count the number of syllables\n",
        "syll = lambda w:len(''.join(c if c in\"aeiouy\"else' 'for c in w.rstrip('e')).split())\n",
        "\n",
        "i=0\n",
        "for row in tsvin:\n",
        "    id=row[0] # id del párrafo donde ocurre la palabra\n",
        "    sentence=row[1] #oración\n",
        "    word=row[4] #palabra a clasificar\n",
        "\n",
        "    len_word=len(word)\n",
        "    num_syl=syll(word)\n",
        "    len_sen=len(sentence)\n",
        "    \n",
        "    print(word,len_word,num_syl,len_sen)\n",
        "    i+=1\n",
        "    if i==50:\n",
        "        break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Aboriginal', 10, 4, 358)\n",
            "('passing', 7, 2, 358)\n",
            "('land', 4, 1, 358)\n",
            "('legislaton', 10, 4, 358)\n",
            "('rights', 6, 1, 358)\n",
            "('preceded', 8, 3, 358)\n",
            "('Australia', 9, 3, 358)\n",
            "('Aboriginal', 10, 4, 358)\n",
            "('number', 6, 2, 358)\n",
            "('important', 9, 3, 358)\n",
            "('Stockmen', 8, 2, 358)\n",
            "('protests', 8, 2, 358)\n",
            "('including', 9, 3, 358)\n",
            "('Aboriginal', 10, 4, 358)\n",
            "('Strike', 6, 1, 358)\n",
            "('Yolngu', 6, 2, 358)\n",
            "('Petition', 8, 3, 358)\n",
            "('Bark', 4, 1, 358)\n",
            "('Wave', 4, 1, 358)\n",
            "('Walk-Off', 8, 1, 358)\n",
            "('Hill', 4, 1, 358)\n",
            "('Aboriginal', 10, 4, 358)\n",
            "('Trust', 5, 1, 358)\n",
            "('Lands', 5, 1, 358)\n",
            "('established', 11, 4, 358)\n",
            "('Act', 3, 0, 358)\n",
            "('SA', 2, 0, 358)\n",
            "('South', 5, 1, 358)\n",
            "('Australian', 10, 3, 358)\n",
            "('Aboriginal', 10, 4, 358)\n",
            "('Lands', 5, 1, 358)\n",
            "('Trust', 5, 1, 358)\n",
            "('indigenous', 10, 4, 243)\n",
            "('However', 7, 3, 243)\n",
            "('Australians', 11, 3, 243)\n",
            "('Australian', 10, 3, 243)\n",
            "('Aborigines', 10, 4, 243)\n",
            "('Torres', 6, 2, 243)\n",
            "('Strait', 6, 1, 243)\n",
            "('Islanders', 9, 2, 243)\n",
            "('politically', 11, 5, 243)\n",
            "('emerged', 7, 3, 243)\n",
            "('active', 6, 2, 243)\n",
            "('movement', 8, 3, 243)\n",
            "('powerful', 8, 3, 243)\n",
            "('recognition', 11, 4, 243)\n",
            "('Aboriginal', 10, 4, 243)\n",
            "('land', 4, 1, 243)\n",
            "('rights', 6, 1, 243)\n",
            "('novel', 5, 2, 124)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm78ucV1iyVo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Another useful feature could be the probability of the word w in a language model. The hypothesis is that complex words tend to be less frequent than simple words, which are usually more common.\n",
        "\n",
        "We used the <a href='http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html'>The CMU-Cambridge Statistical Language Modeling Toolkit v2 </a> to create a language model, which was trained using the **Simple Wikipedia** corpus, with a total of 131,012 articles.  \n",
        "\n",
        "Therefore, the model has been already generated and you can use it directly in this notebook. You can find it in the folder <a href='./LangModels/'>LangModels</a> (you can download from aulaglobal or from https://github.com/isegura/LanguageModel). \n",
        "\n",
        "\n",
        "So, for example:\n",
        "- the file **wiki_1.wngram** contains a list of all possible unigrams that were found in the Simple Wikipedia corpus. For each unigram, its frequency in the corpus is also provided. \n",
        "- Similarly, **wiki_2.wngram** contains a list of all possible bigrams that were found in this corpus. For each bigram, its frequency in the corpus is also provided. \n",
        "\n",
        "First, we load these models into dictionaries. The ngrams will be the keys and the frequencies their corresponding values. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4RAYSAvdxTj",
        "colab_type": "code",
        "outputId": "0fb4f59c-9c0b-4d65-f949-3793b501b97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def loadDic(path): \n",
        "    dic={}\n",
        "    f=open(path,'r')\n",
        "    for line in f:\n",
        "        line=line.strip()\n",
        "        pos=line.rfind(' ')\n",
        "        key=line[0:pos-1]\n",
        "        freq=line[pos+1]\n",
        "        #print(key,freq)\n",
        "        dic[key]=int(freq)\n",
        "    f.close()\n",
        "    return dic\n",
        "print(sst_home)\n",
        "path=sst_home+'LangModels/wiki_1.wngram'\n",
        "unigrams=loadDic(path)\n",
        "totalUnis=sum(unigrams.itervalues())\n",
        "maxValue=max(unigrams.itervalues())\n",
        "\n",
        "print('number of unigrams',len(unigrams.keys()),totalUnis,maxValue)\n",
        "\n",
        "#Add the code to load the other models\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/TESI/6-TextSimplification/\n",
            "('number of unigrams', 105203, 244669, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P_P1sCan5S5",
        "colab_type": "text"
      },
      "source": [
        "Once we have loaded the dictionary, we can already calculate the probability for each word in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkH_bKpdxTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "SCALED=10**4\n",
        "def getProbability(ngram,dic,size):\n",
        "    #pasamos a minúsculas y borramos  blancos de los extremos\n",
        "    ngram=ngram.lower()\n",
        "    \n",
        "    ngram=ngram.strip()\n",
        "    prob=0\n",
        "    try:\n",
        "        prob=dic[ngram]\n",
        "        prob=prob/size\n",
        "    except:\n",
        "        #print(ngram, ' was not found!!!')\n",
        "        pass\n",
        "\n",
        "    prob=prob*SCALED\n",
        "    return prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVP9-7W6ou86",
        "colab_type": "text"
      },
      "source": [
        "Let us to show the features set (including already the probability) for the 50 first words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqQWw7qNoQBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "8789b871-8675-4bea-b81f-4e56124fea04"
      },
      "source": [
        "i=0\n",
        "for row in tsvin:\n",
        "    id=row[0] # id del párrafo donde ocurre la palabra\n",
        "    sentence=row[1] #oración\n",
        "    word=row[4] #palabra a clasificar\n",
        "\n",
        "    len_word=len(word)\n",
        "    num_syl=syll(word)\n",
        "    len_sen=len(sentence)\n",
        "    probability=getProbability(word,unigrams,totalUnis)\n",
        "\n",
        "    print(word,len_word,num_syl,len_sen,probability)\n",
        "    i+=1\n",
        "    if i==50:\n",
        "        break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('major', 5, 2, 180, 0.326972358574237)\n",
            "('key', 3, 1, 180, 0.040871544821779626)\n",
            "('E-flat', 6, 1, 180, 0)\n",
            "('major', 5, 2, 180, 0.326972358574237)\n",
            "('Brahms', 6, 1, 86, 0)\n",
            "('Johannes', 8, 3, 86, 0)\n",
            "('Faur\\xc3\\xa9', 6, 1, 86, 0)\n",
            "('C\\xc3\\xa9sar', 6, 1, 86, 0)\n",
            "('Franck', 6, 1, 86, 0)\n",
            "('Gabriel', 7, 2, 86, 0.040871544821779626)\n",
            "('violin', 6, 2, 86, 0.24522926893067779)\n",
            "('wrote', 5, 1, 86, 0.040871544821779626)\n",
            "('sonatas', 7, 3, 86, 0)\n",
            "('major', 5, 2, 86, 0.326972358574237)\n",
            "('connection', 10, 3, 130, 0.326972358574237)\n",
            "('Kreutzer', 8, 2, 130, 0)\n",
            "('Beethoven', 9, 3, 130, 0)\n",
            "('Cropper', 7, 2, 130, 0)\n",
            "('Sonata', 6, 3, 130, 0.20435772410889813)\n",
            "('Peter', 5, 2, 130, 0.24522926893067779)\n",
            "('major', 5, 2, 130, 0.326972358574237)\n",
            "('fullest', 7, 2, 130, 0)\n",
            "('sounding', 8, 2, 130, 0)\n",
            "('violin', 6, 2, 130, 0.24522926893067779)\n",
            "('key', 3, 1, 130, 0.040871544821779626)\n",
            "('Friedrich', 9, 2, 214, 0)\n",
            "('According', 9, 2, 214, 0)\n",
            "('Christian', 9, 2, 214, 0.1634861792871185)\n",
            "('Schubart', 8, 2, 214, 0)\n",
            "('Daniel', 6, 2, 214, 0.040871544821779626)\n",
            "('major', 5, 2, 214, 0.326972358574237)\n",
            "('suitable', 8, 2, 214, 0)\n",
            "('key', 3, 1, 214, 0.040871544821779626)\n",
            "('declarations', 12, 4, 214, 0)\n",
            "('innocent', 8, 3, 214, 0.2861008137524574)\n",
            "('beloved', 7, 3, 214, 0)\n",
            "('love', 4, 1, 214, 0.040871544821779626)\n",
            "('hope', 4, 1, 214, 0.1634861792871185)\n",
            "('youthful', 8, 2, 214, 0)\n",
            "('parting', 7, 2, 214, 0.040871544821779626)\n",
            "('cheerfulness', 12, 3, 214, 0)\n",
            "('trust', 5, 1, 214, 0.040871544821779626)\n",
            "('God', 3, 1, 214, 0.20435772410889813)\n",
            "('Ain', 3, 1, 153, 0.040871544821779626)\n",
            "('Al', 2, 0, 153, 0.040871544821779626)\n",
            "('oases', 5, 2, 153, 0)\n",
            "('called', 6, 2, 153, 0)\n",
            "('Garden', 6, 2, 153, 0.08174308964355925)\n",
            "('City', 4, 2, 153, 0.08174308964355925)\n",
            "('Gulf', 4, 1, 153, 0.040871544821779626)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsr1cqawpFur",
        "colab_type": "text"
      },
      "source": [
        "## Training a SVM model to classify complex words\n",
        "\n",
        "Now we are going to train a SVM classifier to train a model using the training dataset. Then, we will apply this model on the test dataset to predict its classes and calculate the metrics precision, recall and Fmeasure using sklearn. \n",
        "\n",
        "The input of the SVM classifier will be the representation of the instances. An instance for each word to be classified. Therefore, the input can be represented as a matrix of n rows by m  columns, where n is the number of words (instances or lines in the training dataset) and m is the number of features plus the class . \n",
        "\n",
        "The following cell calculates the total number of instances (number of rows):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y49RUlXTdxTn",
        "colab_type": "code",
        "outputId": "0a93f2d3-3ab9-40ac-d31c-7007d972979b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def getLines(path):\n",
        "    f=open(path,'r') \n",
        "    numExamples = sum(1 for line in f)  # fileObject is your csv.reader\n",
        "    f.close()\n",
        "    return numExamples\n",
        "\n",
        "\n",
        "path=sst_home+'data/Wikipedia_Train1.tsv'\n",
        "print(\"Num. de ejemplos\", getLines(path))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Num. de ejemplos', 4830)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP20xYs4qOwV",
        "colab_type": "text"
      },
      "source": [
        "The following function first create an empty matrix (with dimension 4830 rows and 5 columns = 4 features plus the class). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbY2rpTUdxTo",
        "colab_type": "code",
        "outputId": "ec8e5e29-3a95-4cf5-ccdd-dc4cc601fcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def getMatrix(path):\n",
        "    \n",
        "    numExamples=getLines(path)\n",
        "   \n",
        "    #WArning!!!,if you add more feature, you should increase this value\n",
        "    numFeatures=5 \n",
        "    #create the empty matrix\n",
        "    matrix = np.empty(shape=[numExamples, numFeatures]) \n",
        "    \n",
        "    #open the file\n",
        "    tsvin=open(path,'rb') \n",
        "    tsvin = csv.reader(tsvin, delimiter='\\t')\n",
        "    \n",
        "    indexRow=0\n",
        "    \n",
        "   \n",
        "    for row in tsvin:\n",
        "        id=row[0] # id text\n",
        "        sentence=row[1] #text\n",
        "        word=row[4] #word\n",
        "        class_word=row[9] #class: 1 o 0\n",
        "\n",
        "\n",
        "        len_word=len(word)\n",
        "        num_syl=syll(word)\n",
        "        len_sen=len(sentence)\n",
        "        prob=getProbability(word,unigrams,totalUnis)\n",
        "        \n",
        "        #create a vector with dimension numFeatures\n",
        "        vector_fet= np.arange(numFeatures)\n",
        "        \n",
        "        #we add the features: \n",
        "\n",
        "        vector_fet[0]=len_word\n",
        "        vector_fet[1]=num_syl\n",
        "        vector_fet[2]=len_sen\n",
        "        vector_fet[3]=prob\n",
        "        #el último la clase\n",
        "        vector_fet[4]=class_word\n",
        "        \n",
        "        #por último, reemplazamos el vector para el ejemplo con indexRow prob_2\n",
        "        matrix[indexRow]=vector_fet\n",
        "        \n",
        "        #incrementamos en 1 para poder indicar el índice del siguiente ejemplo\n",
        "        indexRow+=1\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "    return matrix\n",
        "\n",
        "\n",
        "path=sst_home+'data/Wikipedia_Train1.tsv'\n",
        "matrix_train=getMatrix(path)\n",
        "print(matrix_train)\n",
        "\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  8.   3. 144.   0.   1.]\n",
            " [  6.   2. 144.   0.   1.]\n",
            " [  4.   1. 144.   0.   0.]\n",
            " ...\n",
            " [ 10.   4. 189.   0.   0.]\n",
            " [  7.   2. 189.   0.   0.]\n",
            " [  5.   2. 189.   0.   0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl-coi4NsQkW",
        "colab_type": "text"
      },
      "source": [
        "We should also create the matrix for the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bHBoKpSdxTr",
        "colab_type": "code",
        "outputId": "b763b91f-54f1-45c2-b1e8-c9066dbc4a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "path=sst_home+'data/Wikipedia_Dev1.tsv'\n",
        "matrix_test=getMatrix(path)\n",
        "print(matrix_test)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  4.   1. 160.   0.   1.]\n",
            " [ 13.   5. 160.   0.   1.]\n",
            " [  4.   1. 160.   0.   1.]\n",
            " ...\n",
            " [  4.   1. 133.   0.   0.]\n",
            " [  9.   3. 133.   0.   1.]\n",
            " [  4.   1. 133.   0.   1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6e8B3PndxTs",
        "colab_type": "text"
      },
      "source": [
        "### Training and testing the model\n",
        "\n",
        "We can already train the model. In particular, we will use the SVM classifier (we could use any binary classifier). SKlearn provides several implementations for SVM. We will use this: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. \n",
        "\n",
        "The first column of the matrices represent the outputs (classes) for each instance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B51Sl1h5dxTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numCol=matrix_dev.shape[1]\n",
        "\n",
        "X_train=matrix_train[:,0:numCol-1]\n",
        "y_train=matrix_train[:, -1] #last column\n",
        "\n",
        "numCol=matrix_dev.shape[1]\n",
        "\n",
        "X_dev=matrix_test[:,0:numCol-1]\n",
        "y_dev=matrix_test[:, -1] #last column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j4UtxaotThJ",
        "colab_type": "text"
      },
      "source": [
        "Finally, we train our model and use it to predict the outputs for the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K1Hxg6JdxTu",
        "colab_type": "code",
        "outputId": "b9814952-086e-4230-e7fd-17d99ea6c68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "#nos ayudará a obtener muy fácilmente las métricas precisión, recall y f1\n",
        "from sklearn.metrics import precision_recall_fscore_support as pr\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "#we could add other classifiers.... \n",
        "classifiers = [\n",
        "    svm.SVC()\n",
        "]\n",
        "\n",
        "#en nuestro caso sólo se ejecutará una vez, porque sólo tnemos un algoritmo\n",
        "for item in classifiers:\n",
        "    print(item)\n",
        "    clf = item\n",
        "    #entrenamos\n",
        "    clf.fit(X_train, y_train)\n",
        "    #predicimos\n",
        "    predictions=clf.predict(X_dev)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    print(classification_report(y_dev, predictions))\n",
        "\n",
        "    #obtenemos precisión, recall y f1 comparando el gold standard (y_dev) con las predicciones\n",
        "    #bPrecis, bRecall, bFscore, bSupport = pr(y_dev, predicted, average='binary')\n",
        "    #mostramos resultados\n",
        "    #print(bPrecis, bRecall, bFscore, bSupport)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
            "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
            "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
            "  shrinking=True, tol=0.001, verbose=False)\n",
            "\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.73      0.70       340\n",
            "         1.0       0.61      0.55      0.58       265\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       605\n",
            "   macro avg       0.64      0.64      0.64       605\n",
            "weighted avg       0.65      0.65      0.65       605\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6AkGWKodxTv",
        "colab_type": "text"
      },
      "source": [
        "### G1: a metric for text simplification\n",
        "\n",
        "In  CWI 2018 Shared task, the teams also used an additional metric to compare their sytems. \n",
        "\n",
        "Please, look this metric and implement it for our previous classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4yIj58qutma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}