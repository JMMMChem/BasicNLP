{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZfZs1_I1HSi",
        "colab_type": "text"
      },
      "source": [
        "# NLTK\n",
        "\n",
        "This tutorial is based on https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n",
        "\n",
        "NLTK (Natural Language Toolkit) is a Python library to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources.\n",
        "\n",
        "Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
        "\n",
        "Best of all, NLTK is a free, open source, community-driven project.\n",
        "\n",
        "In this tutorial, we’ll cover the following NLP basic tasks:\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization\n",
        "- Text Lemmatization and Stemming\n",
        "- Stop Words\n",
        "- Regex\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOPFoYYF0q1L",
        "colab_type": "text"
      },
      "source": [
        "## Installing NLTK\n",
        "\n",
        "First we need to install NLTK. Also, we need to download the package 'punkt', which we will use to parse the texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOMjK0ZkrgIU",
        "colab_type": "code",
        "outputId": "a6e032a7-7347-4a81-cf24-e68cd01da7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /root/.local/lib/python3.6/site-packages (3.4.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRssgMxS09v8",
        "colab_type": "text"
      },
      "source": [
        "## Sentence tokenization\n",
        "\n",
        "It is also called sentence segmentation.\n",
        "\n",
        "It is the problem of dividing a string of written language into its component sentences. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark. However, even in English, this problem is not trivial due to the use of full stop character for abbreviations. NLTK does that job for us, so don’t worry too much for the details for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1KtkYDhifyg",
        "colab_type": "code",
        "outputId": "14ee4d8f-a446-47e3-dd27-3a1e26d26e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "text='Billy always listens to his mother. He always does what she says. If his mother says, \"Brush your teeth,\" Billy brushes his teeth.'\n",
        "text +='If his mother says, \"Go to bed,\" Billy goes to bed. Billy is a very good boy. A good boy listens to his mother. His mother does not have to ask him again. She asks him to do something one time, and she does not ask again. '\n",
        "text +='Billy is a good boy. He does what his mother asks the first time. She does not have to ask again. She tells Billy, \"You are my best child.\" Of course Billy is her best child. Billy is her only child.'\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Billy always listens to his mother.\n",
            "\n",
            "He always does what she says.\n",
            "\n",
            "If his mother says, \"Brush your teeth,\" Billy brushes his teeth.If his mother says, \"Go to bed,\" Billy goes to bed.\n",
            "\n",
            "Billy is a very good boy.\n",
            "\n",
            "A good boy listens to his mother.\n",
            "\n",
            "His mother does not have to ask him again.\n",
            "\n",
            "She asks him to do something one time, and she does not ask again.\n",
            "\n",
            "Billy is a good boy.\n",
            "\n",
            "He does what his mother asks the first time.\n",
            "\n",
            "She does not have to ask again.\n",
            "\n",
            "She tells Billy, \"You are my best child.\"\n",
            "\n",
            "Of course Billy is her best child.\n",
            "\n",
            "Billy is her only child.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ssxWRk1HTI",
        "colab_type": "text"
      },
      "source": [
        "## Sentence splitting for other languages\n",
        "\n",
        "Sentence splitting is also possible for other languages than English. You can see that it does not work any well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCoAxDbd1HTK",
        "colab_type": "code",
        "outputId": "5804b771-50b6-492b-af09-f6f6dc4d773f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tokenizer_es = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
        "text='Esta es la asignatura TESI. Este es un curso de NLP. El segundo tema está dedicado a NLTK, Spacy y expresiones regulares.'\n",
        "sentences=tokenizer_es.tokenize(text)\n",
        "print(sentences)\n",
        "for s in sentences:\n",
        "  print(s)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Esta es la asignatura TESI.', 'Este es un curso de NLP.', 'El segundo tema está dedicado a NLTK, Spacy y expresiones regulares.']\n",
            "Esta es la asignatura TESI.\n",
            "Este es un curso de NLP.\n",
            "El segundo tema está dedicado a NLTK, Spacy y expresiones regulares.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pMqW0OviksL",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the task of spliting the text into words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSklTFqY1HSl",
        "colab_type": "code",
        "outputId": "5ca49b92-f9cb-4d2f-d0db-3ff82ae43753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text=\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing\"\n",
        "tokens=[t for t in text.split()]\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr.', \"O'Neill\", 'thinks', 'that', 'the', \"boys'\", 'stories', 'about', \"Chile's\", 'capital', \"aren't\", 'amusing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErhjCtOP1HSw",
        "colab_type": "text"
      },
      "source": [
        "NLTK provides a method to perform this task. Please, observe the tokens are different from those obtained using the previous split method. \n",
        "\n",
        "First, you need to download the NLTK's resources. Please, run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A9HHd5t1HSx",
        "colab_type": "code",
        "outputId": "7ca297a7-647b-428e-8d57-e6cf3e368999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens=nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MdVY1P-g_3x",
        "colab_type": "text"
      },
      "source": [
        "NLTK includes several tokenizers. The following shows compare two of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjN2fZTbhFc9",
        "colab_type": "code",
        "outputId": "858de7b2-dae6-4a33-bfc6-7279ff8cd48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "text=\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing\"\n",
        "\n",
        "tokens=nltk.word_tokenize(text)\n",
        "print('default tokenizer:', tokens)\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "pu_tokenizer = WordPunctTokenizer()\n",
        "tokens=pu_tokenizer.tokenize(text)\n",
        "print('tokenizer for puntuactions:', tokens)\n",
        "\n",
        "\n",
        "#from nltk.parse.corenlp import CoreNLPParser\n",
        "#stanford = CoreNLPParser(url='http://localhost:9000')\n",
        "#print(list(stanford.tokenize('This is a foo bar sentence.')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default tokenizer: ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing']\n",
            "tokenizer for puntuactions: ['Mr', '.', 'O', \"'\", 'Neill', 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'\", 's', 'capital', 'aren', \"'\", 't', 'amusing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogYTNt791HTk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Stemming y Lematización\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "Examples:\n",
        "- am, are, is => be\n",
        "- dog, dogs, dog’s, dogs’ => dog\n",
        "\n",
        "\n",
        "Thus, the following sentence 'the boy’s dogs are different sizes' will be translated to 'the boy dog be differ size'.\n",
        "\n",
        "Stemming and lemmatization are special cases of normalization. However, they are different from each other.\n",
        "\n",
        "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "The difference is that a stemmer operates without knowledge of the context, and therefore cannot understand the difference between words which have different meaning depending on part of speech. But the stemmers also have some advantages, they are easier to implement and usually run faster. Also, the reduced “accuracy” may not matter for some applications.\n",
        "\n",
        "Examples:\n",
        "- The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
        "\n",
        "- The word “play” is the base form for the word “playing”, and hence this is matched in both stemming and lemmatization.\n",
        "\n",
        "- The word “meeting” can be either the base form of a noun or a form of a verb (“to meet”) depending on the context; e.g., “in our last meeting” or “We are meeting again tomorrow”. \n",
        "\n",
        "Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.\n",
        "\n",
        "After we know what’s the difference, let’s see some examples using the NLTK tool.\n",
        "\n",
        "## Lemmatization \n",
        "\n",
        "It is the morphological analysis of a word that returns its lemma. For example, 'walk', 'walked', 'walks', 'walking' share the same base form or lemma: 'walk'.\n",
        "\n",
        " In many NLP applications (such as Information retrieval or question answering) it is very common the use of lemmas instead of tokens in order to to handle  the huge linguistical variations of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxsVp0Vn1HTl",
        "colab_type": "code",
        "outputId": "0c856c2d-4367-4007-d5cd-7096fd5d0f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "sentence='The boys have listened to lots of sad stories.'\n",
        "tokens=nltk.word_tokenize(sentence)\n",
        "lematizer = WordNetLemmatizer()\n",
        "print('Token:\\t\\tLemma:')\n",
        "for t in tokens:\n",
        "    print(t,'\\t\\t',lematizer.lemmatize(t))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token:\t\tLemma:\n",
            "The \t\t The\n",
            "boys \t\t boy\n",
            "have \t\t have\n",
            "listened \t\t listened\n",
            "to \t\t to\n",
            "lots \t\t lot\n",
            "of \t\t of\n",
            "sad \t\t sad\n",
            "stories \t\t story\n",
            ". \t\t .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw3TXFyk1HTr",
        "colab_type": "text"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "It is the process of reducing inflected words to their root (stem). For example, 'fish' is the stem for the following words: 'fishing', 'fished', and 'fisher'.\n",
        "\n",
        "Both processes help to reduce the lexical variability of natural language. In many NLP applications (such as Information retrieval or question answering) it is very common the use of stems instead of tokens. \n",
        "\n",
        "NTLK provides methods to implement these processes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRg-6oeF1HTs",
        "colab_type": "code",
        "outputId": "fc7115b5-b5da-4afc-e7a8-1796d334fb06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "for t in tokens:\n",
        "    print(t,'\\t\\t',porter.stem(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The \t\t the\n",
            "boys \t\t boy\n",
            "have \t\t have\n",
            "listened \t\t listen\n",
            "to \t\t to\n",
            "lots \t\t lot\n",
            "of \t\t of\n",
            "sad \t\t sad\n",
            "stories \t\t stori\n",
            ". \t\t .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_KLlOo0kga9",
        "colab_type": "code",
        "outputId": "19cbf8e3-ef9e-48e3-e336-5c9bb4eb65c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "word='seen'\n",
        "\n",
        "print(word, \" Stemmer:\", stemmer.stem(word))\n",
        "print(word, \"Lemmatizer:\", lemmatizer.lemmatize(word, wordnet.VERB))\n",
        "print()\n",
        "word='drove'\n",
        "\n",
        "print(word, \" Stemmer:\", stemmer.stem(word))\n",
        "print(word, \"Lemmatizer:\", lemmatizer.lemmatize(word, wordnet.VERB))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "seen  Stemmer: seen\n",
            "seen Lemmatizer: see\n",
            "\n",
            "drove  Stemmer: drove\n",
            "drove Lemmatizer: drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPJrRhQskmK4",
        "colab_type": "text"
      },
      "source": [
        "## Stopwords \n",
        "\n",
        "Stopwords are irrelevant words (common words such as “and”, “the”, “a” in a language that do no offer semantic meaning). These word usually add a lot of noise when we apply machine learning algorithms. So we must remove them.\n",
        "\n",
        "The list of the stop words can change depending on your application.\n",
        "\n",
        "The NLTK tool has a predefined list of stopwords that refers to the most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seujp-10kpLR",
        "colab_type": "code",
        "outputId": "4594c90c-fd7e-4c9f-aa12-f03e1a8e0bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SlxKpeLkz3H",
        "colab_type": "text"
      },
      "source": [
        "This code shows how to remove stopwords from a sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP5Bmp_Kkugn",
        "colab_type": "code",
        "outputId": "10010b45-4412-4cad-86e1-f4d8b95fb388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "sentence = \"Backgammon is one of the oldest known board games.\"\n",
        "\n",
        "words = nltk.word_tokenize(sentence)\n",
        "without_stop_words = [word for word in words if not word in stop_words]\n",
        "print(without_stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCPV4be11HTd",
        "colab_type": "text"
      },
      "source": [
        "## PoS tagging \n",
        "\n",
        "Classify words into their lexical categories (part-of-speech tags). Pos tags provide information very useful for tasks such as Named Entity Recognition or Relation extraction. \n",
        "First, you must tokenize the text. Then, the method **pos_tag** provides the PoS tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Ypz7qJ1HTe",
        "colab_type": "code",
        "outputId": "89f9ddd8-d649-4ccb-9574-bf957140cd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "text=\"At least four people were dead after a man began shooting at a synagogue in the Squirrel Hill neighbourhood of Pittsburgh on Saturday.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags=nltk.pos_tag(tokens)\n",
        "print(tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('At', 'IN'), ('least', 'JJS'), ('four', 'CD'), ('people', 'NNS'), ('were', 'VBD'), ('dead', 'JJ'), ('after', 'IN'), ('a', 'DT'), ('man', 'NN'), ('began', 'VBD'), ('shooting', 'VBG'), ('at', 'IN'), ('a', 'DT'), ('synagogue', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Squirrel', 'NNP'), ('Hill', 'NNP'), ('neighbourhood', 'NN'), ('of', 'IN'), ('Pittsburgh', 'NNP'), ('on', 'IN'), ('Saturday', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxts6eqs9e-",
        "colab_type": "text"
      },
      "source": [
        "## Named Entity Recognition \n",
        "\n",
        "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). \n",
        "\n",
        "The possible categories are: \n",
        "PERSON, ORGANIZATION, and GPE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdmesUb4swU3",
        "colab_type": "code",
        "outputId": "02f24221-d1ef-45f4-fd80-6cc3a2cb4f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "print(nltk.ne_chunk(tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  At/IN\n",
            "  least/JJS\n",
            "  four/CD\n",
            "  people/NNS\n",
            "  were/VBD\n",
            "  dead/JJ\n",
            "  after/IN\n",
            "  a/DT\n",
            "  man/NN\n",
            "  began/VBD\n",
            "  shooting/VBG\n",
            "  at/IN\n",
            "  a/DT\n",
            "  synagogue/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Squirrel/NNP Hill/NNP)\n",
            "  neighbourhood/NN\n",
            "  of/IN\n",
            "  (GPE Pittsburgh/NNP)\n",
            "  on/IN\n",
            "  Saturday/NNP\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}