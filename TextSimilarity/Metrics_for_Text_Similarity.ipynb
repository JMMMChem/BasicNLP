{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metrics for Text Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtMxAi7TD61xyRtDvP6pxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/TextSimilarity/blob/master/Metrics_for_Text_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsEh5Bumm3lJ",
        "colab_type": "text"
      },
      "source": [
        "# Metrics for Text Similarity\n",
        "\n",
        "Computing the similarity between texts is necessary in many tasks such as search engines (which have to provide relevant documents for a given query) or customer services (which should be able to understand semantically similar queries from users and provide a uniform response). For example, if the user asks “What has happened to my delivery?” or “What is wrong with my shipping?”, the user will expect the same response.\n",
        "\n",
        "The goal of text similarity is to determine how ‘close’ two pieces of text are \n",
        "in (1) meaning (**semantic similarity**) or (2) surface closeness (**lexical similarity**). \n",
        "For instance, how similar are the phrases *the dog bites the man* with *the man bites the dog* by just looking at the words?. These two phrases are similar as they have the same words. However, these sentences have very different meanings. \n",
        "\n",
        "We are to study two different metrics: (1) Jaccard similarity, which is useful to measure the lexical similarity, and (2) cosine distance, which takes into account the contexts of the words and is useful to measure the semantic similarity. \n",
        "\n",
        "\n",
        "\n",
        "## Jaccard similarity\n",
        "\n",
        "It is the intersection over union is defined as size of intersection divided by size of union of two sets. Let’s take example of two sentences:\n",
        "\n",
        "\n",
        "Sentence 1: AI is our friend and it has been friendly\n",
        "\n",
        "Sentence 2: AI and humans have always been friendly\n",
        "\n",
        "<img src='https://miro.medium.com/max/926/1*NSK8ERXexyIZ_SRaxioFEg.png'/>\n",
        "\n",
        "In order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, “friend” and “friendly” will both become “friend”, “has” and “have” will both become “has”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpJpPa8h4ebc",
        "colab_type": "code",
        "outputId": "dff22418-6daa-4f61-fc8a-c3f10b2dd7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy\n",
        "print('spaCy Version: %s' % (spacy.__version__))\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess(text):\n",
        "  \"\"\"removes the stop words and returns the list of lemmas\"\"\"\n",
        "  doc = spacy_nlp(text)\n",
        "  tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n",
        "  return tokens\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy Version: 2.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUBrXBYW3lXa",
        "colab_type": "code",
        "outputId": "e78f4bb4-8f3e-4278-ae04-3c5cba1a711d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return float(intersection) / union\n",
        "\n",
        "\n",
        "text1=\"AI is our friend and it has been friendly\"\n",
        "text2=\"AI and humans have always been friendly\"\n",
        "tokens1=preprocess(text1)\n",
        "tokens2=preprocess(text2)\n",
        "\n",
        "\n",
        "\n",
        "jaccard_similarity(tokens1,tokens2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rthROB5q5HLW",
        "colab_type": "text"
      },
      "source": [
        "The following texts do not have any word in common, so its jaccard similarity is 0. However, their meanings are very similar. Therefore, Jaccard similarity is neither able to capture semantic similarity nor lexical semantic of these two sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxuu0jIC4Eu4",
        "colab_type": "code",
        "outputId": "d815d5e7-c081-4e30-9c00-8cfdf4241e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text1=\"President greets the press in Chicago\"\n",
        "text2=\"Obama speaks in Illinois\"\n",
        "\n",
        "tokens1=preprocess(text1)\n",
        "tokens2=preprocess(text2)\n",
        "\n",
        "jaccard_similarity(tokens1,tokens2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5IhSx03mFd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Cosine distance\n",
        "Instead of doing a word for word comparison, we also need to pay attention to context in order to capture more of the semantics. Word embeddings can allow us to capture the semantic relations between words. \n",
        "\n",
        "Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. The smaller the angle, higher the cosine similarity.\n",
        "\n",
        "\n",
        "###Exercise### \n",
        "\n",
        "In this exercise, you should define a function that allows us to measure the similarity between two documents (or sentences) based on the cosine distance.\n",
        "\n",
        "The idea is to use a pre-trained word embedding model and the cosine distance. You can load a model using the gensim model (for example, the pre-trained word embedding model **GoogleNews-vectors-negative300.bin**, which we have used in the notebook about Gensim). A document could be represented by the mean vectors of its word embeddings. Previuosly, the texts should be cleaned by removing stopwords and their tokens should be normalized (by lemmatization) to reduce noise.  \n",
        "\n",
        "\n",
        "The **numpy** modules provides a function, **mean**, to calculate the mean vector of a set of vectors (see https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html).\n",
        "\n",
        "To obtain the cosine distance between two vectors, you can use the **scipy** library, in particular, its function cosine (see \n",
        "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9mASgu4Mw5R",
        "colab_type": "text"
      },
      "source": [
        "First, we load the word embedding model by using gensim. This process can take some minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0evZqCfMzuV",
        "colab_type": "code",
        "outputId": "93f9f1c3-fed4-4efd-8dff-b94af9b0f151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "sst_home='drive/My Drive/Colab Notebooks/'\n",
        "#Modify your folder\n",
        "sst_home += 'TESI/5-TextSimilarity/'\n",
        "\n",
        "!ls 'drive/My Drive/Colab Notebooks/TESI/5-TextSimilarity/data/'\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "# Load vectors directly from the file\n",
        "model = KeyedVectors.load_word2vec_format(sst_home+'data/GoogleNews-vectors-negative300.bin', binary=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "GoogleNews-vectors-negative300.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JcGhQ_7OQDs",
        "colab_type": "text"
      },
      "source": [
        "Implement your solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsXjjDTL7O52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import scipy\n",
        "\n",
        "\n",
        "def cosine_distance_wordembedding(s1, s2):\n",
        "    print(s1)\n",
        "    print(s2)\n",
        "    \n",
        "    #Complete your code\n",
        "    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')\n",
        "    print()\n",
        "\n",
        "text1=\"President greets the press in Chicago\"\n",
        "text2=\"Obama speaks in Illinois\"\n",
        "\n",
        "cosine_distance_wordembedding(text1, text2)\n",
        "\n",
        "cosine_distance_wordembedding(text1, text1)\n",
        "\n",
        "text1=\"AI is our friend and it has been friendly\"\n",
        "text2=\"AI and humans have always been friendly\"\n",
        "\n",
        "cosine_distance_wordembedding(text1, text2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}